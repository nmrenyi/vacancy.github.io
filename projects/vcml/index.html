<!DOCTYPE html>
<html lang="en">
<head>
    <title>Visual Concept-Metaconcept Learning</title>
    <meta name="description" content="Visual Concept-Metaconcept Learning">
    <meta name="keywords" content="MIT,Vision,Scene,Language,Learning,Reasoning,Computer Science">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="shortcut icon" href="static/img/favicon.ico">

    <!--[if lt IE 9]>
      <script src="http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">
body{font-family:"Open Sans",Segoe,"Segoe UI","Lucida Sans Unicode","Lucida Grande","Avenir","Seravek","Ubuntu","DejaVu Sans","Trebuchet MS",Verdana,Arial,sans-serif}
body{margin:0;padding:0;background-color:#f4f4f4}
#header{background:#eee;opacity:0.95;margin:0 auto;padding:50px 0 0;text-align:center;cursor:default;text-align:center}
#header-container{margin:0 auto;padding: 0 2em;max-width:1600px}

#header-container .col-logo{text-align:left}
#header-container .logo{position:relative;z-index:100;height:50px;margin-top:20px;margin-right:10px}

#header-container .col-info{text-align:left;margin-bottom:20px}
#header-container #title{margin:.525em 0 1.525em;font-size:1.6em;font-weight:800;text-align:center}

.paper-info{display:inline-block;margin:0 auto;text-align:left}
.paper-info-margin{margin-bottom:20px}
.paper-info p, .paper-info h3{line-height:1.6em; margin-bottom: 0em}
.paper-info .title{font-size:16px;color:#333333;font-weight:600}
.paper-info .authors, .paper-info .authors a{color:#666666}
.paper-info .email{color:#666; font-size:14px}
.paper-info .tag{margin:auto auto;padding:0;list-style:none;text-align:left}
.paper-info .tag li{display:inline-block;margin:auto;padding:0 3px 0 0;line-height:10px}
.paper-info .conference, .paper-info .conference a{color:#BB2222;font-weight:600}
.paper-info .note{display:block;color:#666666;text-decoration:none;font-size:14px}

#wave-canvas{display:block;margin:-80px 0 0;width:100%;height:150px}

#content{padding-top:0px;text-align:left}
#content-container{margin:0 auto;padding: 0 2em;max-width:1600px;color:#333333}

#content-container .header{background:#eee;padding:15px 30px 5px;border-bottom:3px solid #dddddd}
#content-container .header .indicator{color:#777777;margin-right:12px}

#content-container .content{background:#ffffff;padding:15px 5px 15px 30px}
#content-container .content .caption{color:#666666}
#content-container .content .bib{color:#666666;font-size:14px;}
#content-container .content .bib pre{margin:0;padding:0;}

#footer{padding:2em 0 0.5em;margin:30px 0 0;background:#ffffff;opacity:0.95;font-size:14px;line-height:12px;text-align:center}
.highlight, .highlight a{color:#BB2222;font-weight:600}
    </style>
</head>
<body>
<div id="main">
    <div id="header">
        <div id="header-container" class="container">
          <div class="row">
            <div class="col-md-12 col-xl-12 col-info">
                <h1 id="title">Visual Concept-Metaconcept Learning</h1>
                <div class="paper-info">
                    <h3 class="title">Visual Concept-Metaconcept Learning</h3>
                    <p class="authors">
                        <a href="http://hanchi.me">Chi Han</a>*,
                        <a href="http://jiayuanm.com/">Jiayuan Mao</a>*,
                        <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
                        <a href="http://cocosci.mit.edu/josh">Joshua B. Tenenbaum</a>, and
                        <a href="http://jiajunwu.com/">Jiajun Wu</a>
                    </p>
                    <span class="note">(*: First two authors contributed equally.)</span>
                    <ul class="tag">
                        <li class="conference"><a href="https://nips.cc/Conferences/2019">NeurIPS 2019</a></li>
                        <li><a href="http://vcml.csail.mit.edu/data/papers/2019NeurIPS-VCML.pdf">Paper</a> /</li>
                        <li style="margin-right:40px"><a href="http://vcml.csail.mit.edu/data/bibtex/2019NeurIPS-VCML.bib">BibTeX</a></li>
                    </ul>
                </div>
            </div>
          </div>

        </div>
        <canvas id="wave-canvas"></canvas>
    </div>
    <div id="content">
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Overview</h4></div>
            <div class="content">

            <p>
            <center>
            <img width="100%" src="data/img/teaser.png"/>
            </center>
            </p>
            <p class="caption">
            Figure 1: The <i>visual concept-metaconcept learner</i> learns concepts and metaconcepts from images and two types of questions. The learned knowledge helps visual concept learning (generalizing to unseen visual concept compositions, or to concepts with limited visual data) and metaconcept generalization (generalizing to relations between unseen pairs of concepts..
            </p>

            <p>
            Humans reason with concepts and metaconcepts: we recognize <i>red</i> and <i>green</i> from visual input; we also understand that they <i>describe the same property of objects</i> (<i>i.e.</i>, the color). In this paper, we propose the visual concept-metaconcept learner (VCML) for joint learning of concepts and metaconcepts from images and associated question-answer pairs. The key is to exploit the bidirectional connection between visual concepts and metaconcepts. Visual representations provide grounding cues for predicting relations between unseen pairs of concepts. Knowing that red and green <i>describe the same property of objects</i>, we generalize to the fact that cube and sphere also <i>describe the same property of objects</i>, since they both categorize the shape of objects. Meanwhile, knowledge about metaconcepts empowers visual concept learning from limited, noisy, and even biased data.
            From just a few examples of <i>purple cubes</i> we can understand a new color <i>purple</i>, which resembles the hue of the cubes instead of the shape of them.
            Evaluation on both synthetic and real-world datasets validates our claims.

            We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them;
            instead, our model learns by simply looking at images and reading paired questions and answers.
            </p>

            </div>
        </div>
        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Resources</h4></div>
            <div class="content">
                <ul>
                    <li>VCML model in <a href="https://github.com/Glaciohound/VCML">[PyTorch (Official)]</a>.</li>
                    <li>
                        <a href="http://vcml.csail.mit.edu/data/dataset_augmentation.tgz">Augmented datasets</a> (CLEVR, GQA, and CUB).
                        Details could be found at <a href="https://github.com/Glaciohound/VCML#prepare-the-dataset">here</a>.
                    </li>
                </ul>
            </div>
        </div>

        <div id="content-container" class="container">
            <div class="header"><h4><span class="indicator">=</span>Related Publications</h4></div>
            <div class="content">
                <div class="paper-info paper-info-margin">
                    <h3 class="title">
                    The Neuro-Symbolic Concept Learner:
                    Interpreting Scenes, Words, and Sentences From Natural Supervision</h3>
                    <p class="authors">
                        <a href="http://jiayuanm.com/">Jiayuan Mao</a>,
                        <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
                        <a href="https://sites.google.com/site/pushmeet/">Pushmeet Kohli</a>,
                        <a href="http://cocosci.mit.edu/josh">Joshua B. Tenenbaum</a>, and
                        <a href="http://jiajunwu.com/">Jiajun Wu</a>
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://nips.cc/Conferences/2018">ICLR 2019</a> (Oral)</li>
                        <li><a href="http://nscl.csail.mit.edu/data/papers/2019ICLR-NSCL.pdf">Paper</a> /</li>
                        <li><a href="http://nscl.csail.mit.edu">Project Page</a> /</li>
                        <li><a href="http://nscl.csail.mit.edu/data/bibtex/2019ICLR-NSCL.bib">BibTeX</a></li>
                        <li class="authors">(* indicates equal contributions)</li>
                    </ul>
                </div>
                <div class="paper-info paper-info-margin">
                    <h3 class="title">Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding</h3>
                    <p class="authors">
                        Kexin Yi*,
                        <a href="http://jiajunwu.com/">Jiajun Wu</a>*,
                        <a href="http://people.csail.mit.edu/ganchuang/">Chuang Gan</a>,
                        <a href="http://web.mit.edu/torralba/www/">Antonio Torralba</a>,
                        <a href="https://sites.google.com/site/pushmeet/">Pushmeet Kohli</a>, and
                        <a href="http://cocosci.mit.edu/josh">Joshua B. Tenenbaum</a>
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="https://nips.cc/Conferences/2018">NeurIPS 2018</a> (Spotlight)</li>
                        <li><a href="http://nsvqa.csail.mit.edu/papers/nsvqa_nips.pdf">Paper</a> /</li>
                        <li><a href="http://nsvqa.csail.mit.edu">Project Page</a> /</li>
                        <li><a href="http://nsvqa.csail.mit.edu/bibtex/nsvqa_nips.bib">BibTeX</a></li>
                        <li class="authors">(* indicates equal contributions)</li>
                    </ul>
                </div>
                <div class="paper-info">
                    <h3 class="title">Neural Scene De-rendering</h3>
                    <p class="authors">
                        <a href="http://jiajunwu.com">Jiajun Wu</a>,
                        <a href="http://cocosci.mit.edu/josh">Joshua B. Tenenbaum</a>, and
                        <a href="https://sites.google.com/site/pushmeet">Pushmeet Kohli</a>
                    </p>
                    <ul class="tag">
                        <li class="conference"><a href="http://cvpr2017.thecvf.com/">CVPR 2017</a></li>
                        <li><a href="http://nsd.csail.mit.edu/papers/nsd_cvpr.pdf">Paper</a> /</li>
                        <li><a href="http://nsd.csail.mit.edu">Project Page /</a></li>
                        <li><a href="http://nsd.csail.mit.edu/bibtex/nsd_cvpr.bib">BibTeX</a></li>
                    </ul>
                </div>
            </div>
        </div>
    </div>
    <div id="footer">
        <p>The Paper Authors © 2019</p>
    </div>
</div>

<!-- jQuery first, then Tether, then Bootstrap JS. -->
<script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"
        integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js"
        integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
<script type="text/javascript" src="static/js/jquery.color.min.js"></script>
<script type="text/javascript" src="static/js/wave.js"></script>
<script type="text/javascript">
$(function() {
    targetColor = $("#title").css("color")
    animatedLink = function(speed) {
        $(".link-li").hover(function() {
            $(this).find('.icon').animate({
                color: targetColor,
                borderColor: targetColor
            }, speed);
            $(this).find('.caption').animate({
                color: '#798350'
            })
        }, function() {
            $(this).find('.icon').animate({
                borderColor: '#cccccc',
                color: '#cccccc'
            }, speed);
            $(this).find('.caption').animate({
                color: '#cccccc'
            })
        })
    };
    // fullBg();
    animatedLink(400)
});
</script>
</body>
</html>
