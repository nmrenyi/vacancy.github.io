<!DOCTYPE html>
<html lang="en">
<head>
    <title>Neuro-Symbolic Visual Reasoning and Program Synthesis</title>
    <meta name="description" content="Neuro-Symbolic Visual Reasoning and Program Synthesis">
    <meta name="keywords" content="Vision,Program Synthesis,Language,Learning,Reasoning,Computer Science">

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/css/bootstrap.min.css" integrity="sha384-rwoIResjU2yc3z8GV/NPeZWAv56rSmLldC3R/AZzGRnGxQQKnKkoFVhFQhNUwEyJ" crossorigin="anonymous">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!--link rel="shortcut icon" href="static/img/favicon.ico"-->

    <!--[if lt IE 9]>
      <script src="http://cdn.bootcss.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="http://cdn.bootcss.com/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->
    <style type="text/css">
body{font-family:"Open Sans",Segoe,"Segoe UI","Lucida Sans Unicode","Lucida Grande","Avenir","Seravek","Ubuntu","DejaVu Sans","Trebuchet MS",Verdana,Arial,sans-serif}
body{margin:0;padding:0}
#header{background:#fafafa;opacity:0.95;margin:0 auto;padding:50px 0 0;text-align:center;cursor:default;text-align:center}
#header-container{margin:0 auto;padding: 0 2em;max-width:1600px}

#header-container .col-logo{text-align:left}
#header-container .logo{position:relative;z-index:100;height:50px;margin-top:20px;margin-right:10px}

#header-container .col-info{text-align:left;margin-bottom:20px}
#header-container #title{margin:.525em 0;font-size:2.4em;font-weight:500;text-align:center}
#header-container .subtitle{color:#333333; font-size:20px; text-align:center; font-weight:normal}
#header-container .datetime{color:#333333; font-size:20px; text-align:center; font-weight:normal;text-decoration:line-through}
#header-container .youtube{color:#333333; font-size:20px; text-align:center; font-weight:600}
#header-container .youtube a,
#header-container .youtube a:hover,
#header-container .youtube a:focus {color:#BB2222; font-size:20px; text-align:center; font-weight:600}

.paper-info{display:inline-block;margin:0 auto;text-align:left}
.paper-info-margin{margin-bottom:20px}
.paper-info p, .paper-info h3{line-height:1.6em; margin-bottom: 0em}
.paper-info .title{font-size:16px;color:#333333;font-weight:600}
.paper-info .authors, .paper-info .authors a{color:#666666}
.paper-info .email{color:#666; font-size:14px}
.paper-info .tag{margin:auto auto;padding:0;list-style:none;text-align:left}
.paper-info .tag li{display:inline-block;margin:auto;padding:0 3px 0 0;line-height:10px}
.paper-info .conference, .paper-info .conference a{color:#BB2222;font-weight:600}

#wave-canvas{display:block;margin:-80px 0 0;width:100%;height:150px}

#content{padding-top:0px;text-align:left}
.content-container{margin:0 auto 10px; padding: 0 2em;max-width:1600px;color:#333333}

.content-container .header{padding:15px 30px 5px;border-bottom:1px solid #dddddd}
.content-container .header .indicator{color:#777777;margin-right:12px}

.content-container .main{background:#ffffff;padding:15px 5px 15px 30px}
.content-container .main .caption{color:#666666}
.content-container .main .bib{color:#666666;font-size:14px;}
.content-container .main .bib pre{margin:0;padding:0;}

.overview{font-size:18px}
.schedule-table th{width:20%}
.reference-list {color: #666666;}
.reference-list .number{min-width:40px;text-align:right;padding-right:10px}
.speakers-list img{width:100%;max-width:148.5px;}
.speakers-list .row{margin-bottom:20px}
.organizers-list>div {width:100%;flex-wrap:wrap}
.organizers-list .item {width:200px;margin-right:50px;text-align:left}

#footer{padding:2em 0 0.5em;margin:30px 0 0;background:#ffffff;opacity:0.95;font-size:14px;line-height:12px;text-align:center}
.highlight, .highlight a{color:#BB2222;font-weight:600}
    </style>
</head>
<body>
<div id="main">
    <div id="header">
        <div id="header-container" class="container">
          <div class="row">
            <div class="col-md-12 col-xl-12 col-info">
                <h1 id="title">Neuro-Symbolic Visual Reasoning and Program Synthesis</h1>
                <h4 class="subtitle">CVPR 2020 Tutorial</h4>
                <h4 class="datetime">June 14, 2020&nbsp;&nbsp;9:00 AM - 1:00 PM (Pacific Time)</h4>
                <h4 class="youtube">Video recordings are available at <a href="https://www.youtube.com/watch?v=Opunfo422uQ&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=1">this Youtube link</a>!</h4>
            </div>
          </div>

        </div>
        <canvas id="wave-canvas"></canvas>
    </div>
    <div id="content">
        <div class="content-container container">
            <div class="header"><h4>Overview</h4></div>
            <div class="main overview">
                <p>
                Recent advances in deep learning gave rise to highly expressive models achieving remarkable results on visual perception tasks such as object, action and scene recognition. However, it is widely accepted that in order to develop truly intelligent systems, we need to bridge the gap between perception and cognition. Highly cognitive tasks such as planning, abstracting, reasoning and explaining are typically associated with symbolic systems which do not scale to the complex high-dimensional visual world. The relatively new field of neuro-symbolic computation proposes to combine the strengths of deep models with symbolic approaches, by using the former to learn disentangled, interpretable, low-dimensional representations which significantly reduce the search space for symbolic approaches such as program synthesis (cf. [5,15]). Another reason to study the interplay between neural and symbolic approaches is related to human, and in particular infant, learning. While far from fully understood, there is an increasing body of evidence that similar mechanisms combining low-level perception with high level cognition are at play in the human brain [1,12].
                </p><p>
                This tutorial will bring together researchers from computer vision, graphics, robotics, cognitive science, and developmental psychology to exchange ideas, share recent research results and applications in the emerging field of neuro-symbolic computation, focusing on computer vision.
                </p>
            </div>
        </div>
        <div class="content-container container">
            <div class="header"><h4>Schedule (Pacific Time)</h4></div>
            <div class="main">
                <table class="schedule-table table table-striped">
                  <tbody>
                    <tr> <th scope="row">9:00 AM - 9:10 AM</th>   <td>Opening Remarks: <b>Jiajun Wu</b> [<a href="https://www.youtube.com/watch?v=Opunfo422uQ&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=1" target="_blank">Youtube</a>]</td> </tr>
                    <tr> <th scope="row">9:10 AM - 9:40 AM</th>   <td>Talk 1: <b>Christopher Manning</b>: More-neural Symbolic Concept Learning [<a href="https://www.youtube.com/watch?v=m78qYOdK4Tg&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=2" target="_blank">Youtube</a>]</td> </tr>
                    <tr> <th scope="row">9:40 AM - 10:10 AM</th>  <td>Talk 2: <b>Yejin Choi</b>: Neuro-Symbolic Commonsense Intelligence: Closing the Gap between Perception and Cognition [<a href="https://www.youtube.com/watch?v=llYCXO9Ajj0&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=3" target="_blank">Youtube</a>]</td> </tr>
                    <tr> <th scope="row">10:10 AM - 10:40 AM</th> <td>Talk 3: <b>Jiayuan Mao</b>: Neuro-Symbolic Visual Concept Learning [<a href="https://www.youtube.com/watch?v=3_vLTeDHxas&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=4" target="_blank">Youtube</a>]</td> </tr>
                    <tr> <th scope="row">10:40 AM - 10:50 AM</th> <td>Break 1</td> </tr>
                    <tr> <th scope="row">10:50 AM - 11:20 AM</th> <td>Talk 4: <b>Daniel Ritchie</b>: From Neural to Neurosymbolic 3D Modeling [Recording Unavailable <span style="color:#BB2222">*</span>]</td> </tr>
                    <tr> <th scope="row">11:20 AM - 11:50 AM</th> <td>Talk 5: <b>Kevin Ellis</b>: Learning Languages for Visual Programs [<a href="https://www.youtube.com/watch?v=D0bvynLST7M&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=5" target="_blank">Youtube</a>]</td> </tr>
                    <tr> <th scope="row">11:50 AM - Noon</th>     <td>Break 2</td> </tr>
                    <tr> <th scope="row">Noon - 12:30 PM</th>     <td>Talk 6: <b>Rishabh Singh</b>: Towards Human-like Program Synthesis [<a href="https://www.youtube.com/watch?v=8apjJ-xSDB4&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=6" target="_blank">Youtube</a>]</td></tr>
                    <tr> <th scope="row">12:30 AM - 13:00 PM</th> <td>Talk 7: <b>Xinyun Chen</b>: Neural Program Synthesis for Navigation and Language Understanding [<a href="https://www.youtube.com/watch?v=n7yri4SpzKY&list=PLX0h2D8LfCPHtT0-u6xjYaGFPT6-3cLJ1&index=7" target="_blank">Youtube</a>]</td> </tr>
                  </tbody>
                </table>
                <p><span style="color:#BB2222">*</span>: Video recording for Prof. Daniel Ritchie is unavailable online as requested by the speaker.</p>
            </div>
        </div>

        <div class="content-container container">
            <div class="header"><h4>Speakers</h4></div>
            <div class="main speakers-list">
                 <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/speakers/choi.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Yejin Choi</b> is an associate professor of Paul G. Allen School of Computer Science & Engineering at the University of Washington, adjunct of the Linguistics department, and affiliate of the Center for Statistics and Social Sciences.
                        She is also a senior research manager at the Allen Institute for Artificial Intelligence. Her primary research interests are in the fields of Natural Language Processing, Machine Learning, Artificial Intelligence, with broader interests in Computer Vision and Digital Humanities.
                        </p>
                    </div>
                </div>
                 <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/speakers/manning.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Christopher Manning</b> is the inaugural Thomas M. Siebel Professor in Machine Learning in the Departments of Linguistics and Computer Science at Stanford University, Director of the Stanford Artificial Intelligence Laboratory (SAIL), and an Associate Director of the Stanford Human-Centered Artificial Intelligence Institute (HAI).
                        His research goal is computers that can intelligently process, understand, and generate human language material. Manning is a leader in applying Deep Learning to Natural Language Processing, with well-known research on Tree Recursive Neural Networks, the GloVe model of word vectors, sentiment analysis, neural network dependency parsing, neural machine translation, question answering, and deep language understanding.
                        He also focuses on computational linguistic approaches to parsing, natural language inference and multilingual language processing, including being a principal developer of Stanford Dependencies and Universal Dependencies.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/speakers/ritchie.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Daniel Ritchie</b> is an Assistant Professor of Computer Science at Brown University, where he co-lead the Brown Visual Computing group.
                        Ritchie is broadly interested in the intersection of computer graphics with artificial intelligence and machine learning: he builds intelligent machines that understand the visual world and can help people be visually creative.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/speakers/singh.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Rishabh Singh</b> is a research scientist in the Google Brain team, which works on developing new deep learning architectures for learning programs and program analysis.
                        Singh develop new program synthesis techniques for helping end-users, students, and programmers. Apart from research, he enjoy playing bridge.</p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/speakers/chen.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Xinyun Chen</b> is a Ph.D. candidate at UC Berkeley, working with Prof. Dawn Song. Chen's research lies at the intersection of deep learning, programming languages, and security.
                        Her recent research focuses on neural program synthesis and adversarial machine learning, towards tackling the grand challenges of increasing the accessibility of programming to general users, and enhancing the security and trustworthiness of machine learning models.
                        She received the Facebook Fellowship in 2020.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/organizers/ellis.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Kevin Ellis</b> is a Ph.D. student at MIT, advised by Professors Josh Tenenbaum and Armando Solar-Lezama, working in cognitive AI and program synthesis.
                        Ellis develops algorithms for program induction, which means synthesizing programs from data, and apply these algorithms to problems in artificial intelligence.
                        He Will be starting as an assistant professor in the computer science department at Cornell in summer 2021.
                        </p>
                    </div>
                </div>
                <div class="row">
                    <div class="col-l col-md-12 col-lg-2">
                        <img src="static/organizers/mao.jpg" />
                    </div>
                    <div class="col-r col-md-12 col-lg-10">
                        <p><b>Jiayuan Mao</b> is a Ph.D. student at MIT, advised by Professors Josh Tenenbaum and Leslie Kaelbling.
                        Mao's research focuses on structured knowledge representations that can be transferred among tasks and inductive biases that improve the learning efficiency and generalization.
                        Representative research topics are concept learning, neuro-symbolic reasoning, scene understanding, and language acquisition.
                        </p>
                    </div>
                </div>
            </div>
        </div>

        <div class="content-container container">
            <div class="header"><h4>Organizers</h4></div>
            <div class="main organizers-list">
                <div class="d-flex m-auto">
                    <div class="item">
                        <img src="static/organizers/mao.jpg" width="148.5px" />
                        <p><b>Jiayuan Mao</b><br/>(MIT)</p>
                    </div>
                    <div class="item">
                        <img src="static/organizers/ellis.jpg" width="148.5px" />
                        <p><b>Kevin Ellis</b><br/>(MIT)</p>
                    </div>
                    <div class="item">
                        <img src="static/organizers/gan.jpg" width="148.5px" />
                        <p><b>Chuang Gan</b><br/>(MIT-IBM Watson AI Lab)</p>
                    </div>
                    <div class="item">
                        <img src="static/organizers/wu.jpg" width="148.5px" />
                        <p><b>Jiajun Wu</b><br/>(Stanford)</p>
                    </div>
                    <div class="item">
                        <img src="static/organizers/gutfreund.jpg" width="148.5px" />
                        <p><b>Dan Gutfreund</b><br/>(MIT-IBM Watson AI Lab)</p>
                    </div>
                    <div class="item">
                        <img src="static/organizers/tenenbaum.jpg" width="148.5px" />
                        <p><b>Josh Tenenbaum</b><br/>(MIT)</p>
                    </div>
                </div>
            </div>
        </div>

        <div class="content-container container">
            <div class="header"><h4>References</h4></div>
            <div class="main reference-list">
<div class="d-flex"><div class="number">[1]</div> <div>Ben Deen, Hilary Richardson, Daniel D. Dilks, Atsushi Takahashi, Boris Keil, Lawrence L. Wald, Nancy Kanwisher, and Rebecca Saxe. Organization of high-level visual cortex in human infants. Nature communications,8(1):1–10, 2017.</div></div>
<div class="d-flex"><div class="number">[2]</div> <div>Xuguang Duan, Qi Wu, Chuang Gan, Zhang Yiwei, Wenbing Huang, and Wenwu Zhu. Watch, reason and code: Learning to represent videos using program. In <i>ACM Multimedia</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[3]</div> <div>Kevin Ellis, Maxwell Nye, Yewen Pu, Felix Sosa, Joshua B. Tenenbaum, and Armando Solar-Lezama. Write, execute, assess: Program  synthesis with a RERL. In <i>NeurIPS</i>, 2019</div></div>
<div class="d-flex"><div class="number">[4]</div> <div>Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Joshua B. Tenenbaum. Learning to infer graphics programs from hand-drawn images.  In <i>NeurIPS</i>, 2018.</div></div>
<div class="d-flex"><div class="number">[5]</div> <div>Chuang Gan, Yandong Li, Haoxiang Li, Chen Sun, and Boqing Gong. VQS: Linking segmentations to questions and answers for supervised attention in VQA and question-focused semantic segmentation. In <i>ICCV</i>, 2017.</div></div>
<div class="d-flex"><div class="number">[6]</div> <div>Chi Han, Jiayuan Mao, Chuang Gan, Joshua B. Tenenbaum, and Jiajun Wu. Visual concept metaconcept learner. In <i>NeurIPS</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[7]</div> <div>Yunzhu Li, Jiajun Wu, Russ Tedrake, Joshua B. Tenenbaum, and Antonio Torralba. Learning particle dynamics for manipulating rigid bodies, deformable objects, and fluids. In <i>ICLR</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[8]</div> <div>Yunzhu Li, Jiajun Wu, Jun-Yan Zhu, Joshua B. Tenenbaum, Antonio Torralba, and Russ Tedrake. Propagation networks for model-based controlunder partial observation. In <i>ICRA</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[9]</div> <div>Yunchao Liu, Zheng Wu, Daniel Ritchie, William T. Freeman, Joshua B Tenenbaum, and Jiajun Wu. Learning to describe scenes with programs. In <i>ICLR</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[10]</div> <div>Jiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B. Tenenbaum, and Jiajun Wu. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. In <i>ICLR</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[11]</div> <div>Jiayuan Mao, Xiuming Zhang, William T. Freeman, Joshua B. Tenenbaum,and Jiajun Wu. Program-guided image manipulators. In <i>ICCV</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[12]</div> <div>Elizabeth S. Spelke and Katherine D. Kinzler. Core knowledge. Developmental Science, 10(1):89–96, 2007.</div></div>
<div class="d-flex"><div class="number">[13]</div> <div>Yonglong Tian, Andrew Luo, Xingyuan Sun, Kevin Ellis, William T. Free-man, Joshua B. Tenenbaum, and Jiajun Wu. Learning to infer and execute3d shape programs. In <i>ICLR</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[14]</div> <div>Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma. Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations. In <i>CVPR</i>, 2019.</div></div>
<div class="d-flex"><div class="number">[15]</div> <div>Kexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Joshua B. Tenenbaum. Neural-symbolic VQA: Disentangling reasoning from vision and language understanding. In <i>NeurIPS</i>, 2018.</div></div>
            </div>
        </div>

    </div>
    <div id="footer">
        <p>The Tutorial Organiziers &copy; 2020. <a href="https://accessibility.mit.edu/">Accessibility</a></p>
    </div>
</div>

<!-- jQuery first, then Tether, then Bootstrap JS. -->
<script src="https://code.jquery.com/jquery-3.1.1.min.js" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/tether/1.4.0/js/tether.min.js"
        integrity="sha384-DztdAPBWPRXSA/3eYEEUWrWCy7G5KFbe8fFjk5JAIxUYHKkDx6Qin1DkWx51bBrb" crossorigin="anonymous"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.6/js/bootstrap.min.js"
        integrity="sha384-vBWWzlZJ8ea9aCX4pEW3rVHjgjt7zpkNpZk+02D9phzyeVkE+jo0ieGizqPLForn" crossorigin="anonymous"></script>
<script type="text/javascript" src="static/js/jquery.color.min.js"></script>
<script type="text/javascript" src="static/js/wave.js"></script>
</body>
</html>
